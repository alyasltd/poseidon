{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå P3P with Kneip's Method ‚Äî PyTorch Implementation Demo Step-by-Step\n",
    "\n",
    "This notebook presents a Step-by-Step PyTorch-based implementation of the Perspective-Three-Point (P3P) problem using the approach described in the paper:\n",
    "\n",
    "> **A Novel Parametrization of the Perspective-Three-Point Problem for a Direct Computation of Absolute Camera Position and Orientation**  \n",
    "> by Laurent Kneip, Davide Scaramuzza, Roland Siegwart  \n",
    "\n",
    "We follow the core idea of their method to compute the absolute pose (position and orientation) of a calibrated camera from 3 known 3D‚Äì2D point correspondences.\n",
    "\n",
    "---\n",
    "\n",
    "This demo is designed for educational and validation purposes, and uses synthetic data for full control over the geometry and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Problem Setup\n",
    "\n",
    "We are given:\n",
    "\n",
    "- üì∑ Camera intrinsics matrix **A** (includes focal length and principal point)\n",
    "- üìå 3D world points **P‚ÇÅ**, **P‚ÇÇ**, **P‚ÇÉ** (and **P‚ÇÑ** used to disambiguate P3P solutions)\n",
    "- üìç Corresponding 2D image projections for these 3D points\n",
    "\n",
    "Using this data, we compute the unit direction vectors (feature vectors) that point from the camera center toward each 3D point. These vectors are used as input to the P3P algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Objective\n",
    "\n",
    "Estimate the camera's:\n",
    "\n",
    "- üîÑ Rotation matrix **R** (camera orientation)\n",
    "- üìç Position (camera center) **C** in world coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera(batch_size, device):\n",
    "    fx = 800.0\n",
    "    fy = 800.0\n",
    "    cx = 320.0\n",
    "    cy = 240.0\n",
    "\n",
    "    A_single = torch.tensor([\n",
    "        [fx, 0, cx],\n",
    "        [0, fy, cy],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=torch.float32, device=device) #(3, 3)\n",
    "\n",
    "    A = A_single.unsqueeze(0).repeat(batch_size, 1, 1) # (B, 3, 3)\n",
    "    return A\n",
    "\n",
    "def rotation_matrix(batch_size, device):\n",
    "    R_single = torch.tensor([\n",
    "        [1, 0, 0],\n",
    "        [0, -1, 0],\n",
    "        [0, 0, -1]\n",
    "    ], dtype=torch.float32, device=device) # (3,3)\n",
    "\n",
    "    R = R_single.unsqueeze(0).repeat(batch_size, 1, 1) # (B, 3, 3)\n",
    "    return R\n",
    "\n",
    "def camera_position(batch_size, device):\n",
    "    C_single = torch.tensor([[0, 0, 6]], dtype=torch.float32, device=device) # (1, 3)\n",
    "    C = C_single.repeat(batch_size, 1, 1)  # (B, 1, 3) \n",
    "    return C\n",
    "\n",
    "\n",
    "A = camera(batch_size, device) \n",
    "R = rotation_matrix(batch_size, device)\n",
    "C = camera_position(batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Step 2: Generate Synthetic Data\n",
    "\n",
    "To validate our P3P implementation in a controlled environment, we generate synthetic data:\n",
    "\n",
    "-  Create 3 3D points in world coordinates\n",
    "- üì∑ based on the predefined rotation matrix (R) and position (C)\n",
    "- üîÅ Use the camera projection model to project the 3D points onto the image plane:\n",
    "  \n",
    "$$\\textbf{p}_i = A [R \\,|\\, -RC] \\cdot \\textbf{P}_i$$\n",
    "\n",
    "where $\\textbf{p}_i$ is the corresponding 2D point of the 3D point $\\textbf{P}_i$\n",
    "\n",
    "-  Normalize the projected 2D points (homogeneous division)\n",
    "- ‚úÖ Store the resulting 2D coordinates as the observed image points\n",
    "\n",
    "This synthetic setup allows us to test our algorithm without noise or real-world uncertainties.\n",
    "\n",
    "> ‚ÑπÔ∏è Note: In real-world scenarios, the camera pose (R, C) is unknown and must be estimated. The corresponding 2D image points are typically obtained using feature detection and matching methods (e.g., SIFT, ORB, COLMAP, etc.).  \n",
    "> This step of obtaining 2D‚Äì3D correspondences is a necessary preprocessing stage before applying any P3P algorithm.\n",
    "\n",
    "This synthetic setup allows us to test our algorithm without noise or real-world uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected 2D points : tensor([[455.7761, 137.0256],\n",
      "        [187.9764, 145.0786],\n",
      "        [123.5423, 184.6140]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from generate_synthetic_2D3Dpoints import generate_synthetic_2D3Dpoints\n",
    "\n",
    "# Set print precision for better readability\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "P1 = torch.tensor([0.7161, 0.5431, 1.7807], dtype=torch.float32, device=device)\n",
    "P2 = torch.tensor([-1.1643, 0.8371, -1.0551], dtype=torch.float32, device=device)\n",
    "P3 = torch.tensor([-1.5224, 0.4292, -0.1994], dtype=torch.float32, device=device)\n",
    "\n",
    "points2D = generate_synthetic_2D3Dpoints(R, C, A, P1, P2, P3, batch_size, device) #output shape (B, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Step 2 ‚Äî Compute Feature Vectors (Unit Bearing Vectors)\n",
    "\n",
    "This block of code:\n",
    "\n",
    "- Takes the resulting 2D pixel coordinates and transforms them into 3D unit vectors in the camera frame.\n",
    "- For each image point:\n",
    "\n",
    "  - Converts it to homogeneous coordinates. (so adds a dummy dimension)\n",
    "  - Applies the inverse of the intrinsic matrix $A^{-1}$ to back-project the 2D points into the camera frame.\n",
    "  - Normalizes the result to obtain a unit direction vector pointing toward the 3D point.\n",
    "\n",
    "This matches exactly what the Kneip et al. P3P paper assumes:\n",
    "\n",
    "> ‚ÄúWe assume that the unitary vectors f‚ÇÅ, f‚ÇÇ, and f‚ÇÉ‚Äîpointing toward the three considered feature points from the camera frame‚Äîare given.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points in homogeneous coordinates: tensor([[455.7761, 137.0256,   1.0000],\n",
      "        [187.9764, 145.0786,   1.0000],\n",
      "        [123.5423, 184.6140,   1.0000]], device='cuda:0')\n",
      "Normalized feature vectors: tensor([[ 0.1660, -0.1259,  0.9781],\n",
      "        [-0.1617, -0.1163,  0.9800],\n",
      "        [-0.2379, -0.0671,  0.9690]], device='cuda:0')\n",
      "Feature Vectors:\n",
      " tensor([[[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]],\n",
      "\n",
      "        [[ 0.1660, -0.1259,  0.9781],\n",
      "         [-0.1617, -0.1163,  0.9800],\n",
      "         [-0.2379, -0.0671,  0.9690]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from get_feature_vectors import get_feature_vectors\n",
    "\n",
    "featuresVect = get_feature_vectors(points2D, A, batch_size, device)\n",
    "print(\"Feature Vectors:\\n\", featuresVect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ In the end, we have:\n",
    "\n",
    "- 3 known 3D points in world coordinates: $P_1, P_2, P_3$\n",
    "- 3 unit feature vectors in the camera frame: $\\vec{f}_1, \\vec{f}_2, \\vec{f}_3$\n",
    "\n",
    "‚û°Ô∏è These are the two inputs required by the P3P algorithm as formulated by Kneip et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßÆ Polynomial Root Solvers for P3P\n",
    "\n",
    "To complete the Kneip P3P pipeline, we need to solve polynomials of degree 3 and 4. The following functions implement:\n",
    "\n",
    "- A cubic root solver (required as part of Ferrari's method)\n",
    "- A 4th-degree root solver using Ferrari‚Äôs method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoroot.torch.quartic.quartic import (  # type: ignore\n",
    "    polynomial_root_calculation_4th_degree_ferrari,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3P Solution Workflow\n",
    "\n",
    "Now that we have all the required variables (3D points and feature vectors), we can proceed with the P3P pipeline following Kneip‚Äôs method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Store 3D Points  \n",
    "Already defined as:  \n",
    "- P‚ÇÅ, P‚ÇÇ, P‚ÇÉ ‚àà ‚Ñù¬≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched Fixed Points:\n",
      " tensor([[[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]],\n",
      "\n",
      "        [[ 0.7161,  0.5431,  1.7807],\n",
      "         [-1.1643,  0.8371, -1.0551],\n",
      "         [-1.5224,  0.4292, -0.1994]]])\n"
     ]
    }
   ],
   "source": [
    "from get3Dpointsbatch import get_batched_points, generate_random_3D_points\n",
    "\n",
    "P1 = torch.tensor([0.7161, 0.5431, 1.7807], dtype=torch.float32)\n",
    "P2 = torch.tensor([-1.1643, 0.8371, -1.0551], dtype=torch.float32)\n",
    "P3 = torch.tensor([-1.5224, 0.4292, -0.1994], dtype=torch.float32)\n",
    "\n",
    "batched_fixed = get_batched_points(P1, P2, P3, batch_size)\n",
    "print(\"Batched Fixed Points:\\n\", batched_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Store Feature Vectors  \n",
    "Already defined as:  \n",
    "- f‚ÇÅ, f‚ÇÇ, f‚ÇÉ ‚àà ‚Ñù¬≥  \n",
    "These are the unit direction vectors from the camera center toward each 3D point, expressed in the camera frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points in homogeneous coordinates: tensor([[455.7761, 137.0256,   1.0000],\n",
      "        [187.9764, 145.0786,   1.0000],\n",
      "        [123.5423, 184.6140,   1.0000]], device='cuda:0')\n",
      "Normalized feature vectors: tensor([[ 0.1660, -0.1259,  0.9781],\n",
      "        [-0.1617, -0.1163,  0.9800],\n",
      "        [-0.2379, -0.0671,  0.9690]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "featuresVect = get_feature_vectors(points2D, A, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. üì¶ Initialize Solution Storage  \n",
    "There can be up to 4 valid P3P solutions.  \n",
    "We store them in a tensor of shape:\n",
    "\n",
    "$$\n",
    "\\texttt{solutions} \\in \\mathbb{R}^{4 \\times 3 \\times 4}\n",
    "$$\n",
    "\n",
    "Each slice corresponds to one solution:\n",
    "\n",
    "- First column (3√ó1): estimated camera position vector C\n",
    "- Remaining columns (3√ó3): estimated rotation matrix R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solutions = \n",
      " tensor([[[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]], dtype=torch.float64)\n",
      "torch.Size([16, 4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "solutions = torch.zeros((batch_size,4,3,4), dtype=torch.float64)\n",
    "print(\"solutions = \\n\", solutions)\n",
    "print(solutions.shape)  # (4,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ‚ùó Check for Collinearity  \n",
    "Ensure the three 3D points are not collinear:\n",
    "\n",
    "$$\n",
    "(P_2 - P_1) \\times (P_3 - P_1) \\neq 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross product norms: tensor([2.9101, 2.9101, 2.9101, 2.9101, 2.9101, 2.9101, 2.9101, 2.9101, 2.9101,\n",
      "        2.9101, 2.9101, 2.9101, 2.9101, 2.9101, 2.9101, 2.9101])\n",
      "\n",
      "‚úÖ The points are not collinear, we can continue\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from check_collinearity import check_non_collinearity\n",
    "\n",
    "check_non_collinearity(batched_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build Orthonormal Frame in the Image Space\n",
    "\n",
    "To simplify the geometric relationships between the feature direction vectors $\\mathbf{f}_1$, $\\mathbf{f}_2$, and $\\mathbf{f}_3$, we construct an orthonormal basis in the image frame. This local right-handed coordinate system helps reduce the complexity of the subsequent trigonometric calculations.\n",
    "\n",
    "The basis vectors are defined as follows:\n",
    "\n",
    "- $\\mathbf{e}_1 = \\mathbf{f}_1$: the first basis vector is aligned with the direction of the first feature.\n",
    "- $\\mathbf{e}_3 = \\frac{\\mathbf{f}_1 \\times \\mathbf{f}_2}{\\| \\mathbf{f}_1 \\times \\mathbf{f}_2 \\|}$: the third basis vector is the normalized cross product of $\\mathbf{f}_1$ and $\\mathbf{f}_2$, pointing perpendicular to the plane they define.\n",
    "- $\\mathbf{e}_2 = \\mathbf{e}_3 \\times \\mathbf{e}_1$: the second basis vector completes the orthonormal frame, ensuring a right-handed orientation.\n",
    "\n",
    "These three vectors form the matrix $\\mathbf{T} \\in \\mathbb{R}^{3 \\times 3}$, which transforms any vector into this new coordinate system:\n",
    "\n",
    "$$\n",
    "\\mathbf{T} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{e}_1^\\top \\\\\n",
    "\\mathbf{e}_2^\\top \\\\\n",
    "\\mathbf{e}_3^\\top\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In particular, we will use $\\mathbf{T}$ to express $\\mathbf{f}_3$ in this image-aligned frame:\n",
    "\n",
    "$$\n",
    "\\mathbf{f}_3^\\tau = \\mathbf{T} \\mathbf{f}_3.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 =\n",
      " tensor([ 0.1660, -0.1259,  0.9781], device='cuda:0')\n",
      "f2 =\n",
      " tensor([-0.1617, -0.1163,  0.9800], device='cuda:0')\n",
      "f3 =\n",
      " tensor([-0.2379, -0.0671,  0.9690], device='cuda:0')\n",
      "T =\n",
      " tensor([[ 0.1660, -0.1259,  0.9781],\n",
      "        [-0.9857,  0.0088,  0.1684],\n",
      "        [-0.0298, -0.9920, -0.1226]], device='cuda:0')\n",
      "T shape = torch.Size([16, 3, 3])\n",
      "f3_T =\n",
      " tensor([ 0.9166,  0.3971, -0.0452], device='cuda:0')\n",
      "f3_T shape = torch.Size([16, 3])\n",
      "f3_T_positive =\n",
      " tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from get_tau_basis import get_tau_basis_and_f3_proj\n",
    "\n",
    "T, f3_T, f3_T_positive = get_tau_basis_and_f3_proj(featuresVect)\n",
    "\n",
    "print(\"T =\\n\", T[0])\n",
    "print(\"T shape =\", T.shape)  # (B, 3, 3)\n",
    "\n",
    "print(\"f3_T =\\n\", f3_T[0])\n",
    "print(\"f3_T shape =\", f3_T.shape)  # (B, 3)\n",
    "\n",
    "print(\"f3_T_positive =\\n\", f3_T_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Build Orthonormal Frame in the World Space\n",
    "\n",
    "To analyze the 3D geometry in a local coordinate system aligned with the known world points, we construct an orthonormal basis denoted by $\\eta = (\\mathbf{n}_x, \\mathbf{n}_y, \\mathbf{n}_z)$. This basis will serve as a frame attached to the world points $\\mathbf{P}_1, \\mathbf{P}_2, \\mathbf{P}_3$, and will simplify the relation between world and camera coordinates.\n",
    "\n",
    "The construction proceeds as follows:\n",
    "- We define the first axis as the unit vector from $\\mathbf{P}_1$ to $\\mathbf{P}_2$:\n",
    "$\n",
    "\\mathbf{n}_x = \\frac{\\mathbf{P}_2 - \\mathbf{P}_1}{|\\mathbf{P}_2 - \\mathbf{P}_1|}.\n",
    "$\n",
    "- Then, we compute the normal to the plane formed by the three world points using a cross product:\n",
    "$\n",
    "\\mathbf{n}_z = \\frac{\\mathbf{n}_x \\times (\\mathbf{P}_3 - \\mathbf{P}_1)}{|\\mathbf{n}_x \\times (\\mathbf{P}_3 - \\mathbf{P}_1)|}.\n",
    "$\n",
    "- The third axis is obtained by completing the right-handed frame:\n",
    "$\n",
    "\\mathbf{n}_y = \\mathbf{n}_z \\times \\mathbf{n}_x.\n",
    "$\n",
    "\n",
    "These three orthonormal vectors are then arranged as rows to define the transformation matrix $\\mathbf{N} \\in \\mathbb{R}^{3 \\times 3}$:\n",
    "$$\n",
    "\\mathbf{N} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{n}_x^\\top \\\n",
    "\\mathbf{n}_y^\\top \\\n",
    "\\mathbf{n}_z^\\top\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Finally, the 3D point $\\mathbf{P}_3$ is expressed in this new world-aligned coordinate frame by subtracting the origin $\\mathbf{P}_1$ and applying the transformation:\n",
    "$\n",
    "\\mathbf{P}_3^\\eta = \\mathbf{N} (\\mathbf{P}_3 - \\mathbf{P}_1).\n",
    "$\n",
    "\n",
    "This change of basis simplifies subsequent computations, such as aligning world and image vectors to solve the P3P pose estimation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nx =  tensor([-0.5506,  0.0861, -0.8303], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "ny =  tensor([-0.7747, -0.4233,  0.4698], dtype=torch.float64)\n",
      "nz =  tensor([-0.3110,  0.9019,  0.2998], dtype=torch.float64)\n",
      "nx = \n",
      " tensor([[-0.5506,  0.0861, -0.8303]], dtype=torch.float64)\n",
      "torch.Size([1, 3])\n",
      "ny = \n",
      " tensor([[-0.7747, -0.4233,  0.4698]], dtype=torch.float64)\n",
      "nz = \n",
      " tensor([[-0.3110,  0.9019,  0.2998]], dtype=torch.float64)\n",
      "N = \n",
      " tensor([[-0.5506,  0.0861, -0.8303],\n",
      "        [-0.7747, -0.4233,  0.4698],\n",
      "        [-0.3110,  0.9019,  0.2998]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "P3_n = \n",
      " tensor([2.8668, 0.8521, 0.0000], dtype=torch.float64)\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7331/1772977763.py:3: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:62.)\n",
      "  nz = torch.cross(nx,P3-P1)/torch.norm(torch.cross(nx,P3-P1), dim=0)\n"
     ]
    }
   ],
   "source": [
    "# Calculation of vectors of the base Œ∑ = (P1,nx,ny,nz)\n",
    "nx = (P2 - P1)/torch.norm(P2 - P1)      #(3,)\n",
    "nz = torch.cross(nx,P3-P1)/torch.norm(torch.cross(nx,P3-P1), dim=0)  \n",
    "ny = torch.cross(nz,nx)\n",
    "print(\"nx = \", nx)\n",
    "print(nx.shape)  # (3,)\n",
    "print(\"ny = \", ny)\n",
    "print(\"nz = \", nz)\n",
    "\n",
    "# Reshape the vectors to (1,3) for concatenation\n",
    "nx = torch.reshape(nx,(1,3))  # (1,3)\n",
    "ny = torch.reshape(ny,(1,3))\n",
    "nz = torch.reshape(nz,(1,3))\n",
    "print(\"nx = \\n\", nx)\n",
    "print(nx.shape)  # (1*3)\n",
    "print(\"ny = \\n\", ny)\n",
    "print(\"nz = \\n\", nz)\n",
    "\n",
    "# Computation of the matrix N and the world point P3\n",
    "N = torch.cat((nx,ny,nz),dim = 0) # (3*3) T's equivalent in the world coordinate system\n",
    "\n",
    "P3_n = torch.tensordot(N,P3-P1, dims=1) # (3,)\n",
    "\n",
    "print(\"N = \\n\", N)\n",
    "print(N.shape)  # (3,3)\n",
    "print(\"P3_n = \\n\", P3_n)\n",
    "print(P3_n.shape)  # (3,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  Compute Intermediate Variables  \n",
    "These are required for the polynomial setup, we have : \n",
    "- $\\phi_1 = \\frac{f_{3,x}^\\tau}{f_{3,z}^\\tau}$ and $\\phi_2 = \\frac{f_{3,y}^\\tau}{f_{3,z}^\\tau}$ \n",
    "They represent the tangent of the angles that $\\mathbf{f}_3$ makes with the x-axis and y-axis in the image frame (normalized by its z-component). These ratios are used to reduce the 3D geometry into a planar formulation.\n",
    "\n",
    "- we extract the first two components of $\\mathbf{P}_3^\\eta$, the coordinates of point $\\mathbf{P}_3$ in the world-aligned frame $p_1 = (\\mathbf{P}_3^\\eta)_x$ and $p_2 = (\\mathbf{P}_3^\\eta)_y$ that locate $\\mathbf{P}_3$ in the local plane defined by $\\mathbf{P}_1$ and $\\mathbf{P}_2$ in the world.\n",
    "\n",
    "- Computation of the distance between the two known world points $\\mathbf{P}_1$ and $\\mathbf{P}_2$ - $d_{12} = | \\mathbf{P}_2 - \\mathbf{P}_1 |$\n",
    "\n",
    "- Computation of the cosine of the angle $\\beta$ between the two image direction vectors $\\mathbf{f}_1$ and $\\mathbf{f}_2$ \n",
    "- Then computation of its cotangent using the identity:\n",
    "$\\cot(\\beta) = \\frac{\\cos(\\beta)}{\\sqrt{1 - \\cos^2(\\beta)}}$\n",
    "\n",
    "Finally if the angle $\\beta$ is obtuse ($\\cos(\\beta) < 0$), we flip the sign of $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi1 =  tensor(-20.2913, dtype=torch.float64)\n",
      "phi2 =  tensor(-8.7914, dtype=torch.float64)\n",
      "p1 =  tensor(2.8668, dtype=torch.float64)\n",
      "p2 =  tensor(0.8521, dtype=torch.float64)\n",
      "d12 =  tensor(3.4153, dtype=torch.float64)\n",
      "cosBeta =  tensor(0.9463, dtype=torch.float64)\n",
      "b =  tensor(2.9257, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Computation of phi1 et phi2 with 0=x, 1=y, 2=z\n",
    "phi1 = f3_T[0]/f3_T[2]\n",
    "phi2 = f3_T[1]/f3_T[2]\n",
    "print(\"phi1 = \", phi1)\n",
    "print(\"phi2 = \", phi2)\n",
    "\n",
    "# Extraction of p1 and p2 from P3_eta\n",
    "p1 = P3_n[0] #x\n",
    "p2 = P3_n[1] #y\n",
    "print(\"p1 = \", p1)\n",
    "print(\"p2 = \", p2)\n",
    "\n",
    "# Computation of d12\n",
    "d12 = torch.norm(P2-P1) \n",
    "print(\"d12 = \", d12)\n",
    "\n",
    "# Computation of b = cot(beta)\n",
    "cosBeta = torch.dot(f1,f2)/(torch.norm(f1)*torch.norm(f2)) \n",
    "print(\"cosBeta = \", cosBeta)  \n",
    "b = torch.sqrt(1/(1-cosBeta**2)-1)\n",
    "\n",
    "if cosBeta < 0 :\n",
    "    b = -b\n",
    "print(\"b = \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Compute Quartic Polynomial Coefficients\n",
    "\n",
    "Using the previously computed values, we compute the coefficients of a 4th-degree polynomial:\n",
    "\n",
    "$$a_4 x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0 = 0$$\n",
    "\n",
    "These coefficients are derived analytically from the geometric constraints of the P3P problem (as shown in Kneip's paper). This quartic equation will yield possible values of cos(Œ∏), where Œ∏ is a rotation parameter used to recover pose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phi2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mphi2\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m-\u001b[39m phi1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m-\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      2\u001b[0m a3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m d12 \u001b[38;5;241m*\u001b[39m b \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m phi2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m d12 \u001b[38;5;241m*\u001b[39m b \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m phi1 \u001b[38;5;241m*\u001b[39m phi2 \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m d12\n\u001b[1;32m      3\u001b[0m a2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m phi2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m phi2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m d12\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m b\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m phi2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m d12\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m phi2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m phi1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p1 \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m d12 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m phi1 \u001b[38;5;241m*\u001b[39m phi2 \u001b[38;5;241m*\u001b[39m p1 \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m d12 \u001b[38;5;241m*\u001b[39m b \u001b[38;5;241m-\u001b[39m phi1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m phi2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p1 \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m d12 \u001b[38;5;241m-\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m d12\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m b\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'phi2' is not defined"
     ]
    }
   ],
   "source": [
    "a4 = - phi2**2 * p2**4 - phi1**2 * p2**4 - p2**4\n",
    "a3 = 2 * p2**3 * d12 * b + 2 * phi2**2 * p2**3 * d12 * b - 2 * phi1 * phi2 * p2**3 * d12\n",
    "a2 = - phi2**2 * p1**2 * p2**2 - phi2**2 * p2**2 * d12**2 * b**2 - phi2**2 * p2**2 * d12**2 + phi2**2 * p2**4 + phi1**2 * p2 **4 + 2 * p1 * p2**2 * d12 + 2 * phi1 * phi2 * p1 * p2**2 * d12 * b - phi1**2 * p1**2 * p2**2 + 2 * phi2**2 * p1 * p2**2 * d12 - p2**2 * d12**2 * b**2 - 2 * p1**2 * p2**2\n",
    "a1 = 2 * p1**2 * p2 * d12 * b + 2 * phi1 * phi2 * p2**3 * d12 - 2 * phi2**2 * p2**3 * d12 * b - 2 * p1 * p2 * d12**2 * b\n",
    "a0 = - 2 * phi1 * phi2 * p1 * p2**2 * d12 * b + phi2**2 * p2**2 * d12**2 + 2 * p1**3 * d12 - p1**2 * d12**2 + phi2**2 * p1**2 * p2**2 - p1**4 - 2 * phi2**2 * p1 * p2**2 * d12 + phi1**2 * p1**2 * p2**2 + phi2**2 * p2**2 * d12**2 * b**2\n",
    "\n",
    "print(\"a4 = \", a4)\n",
    "print(\"a3 = \", a3)\n",
    "print(\"a2 = \", a2)\n",
    "print(\"a1 = \", a1)\n",
    "print(\"a0 = \", a0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Solve the Quartic Polynomial\n",
    "\n",
    "Use a robust method such as Ferrari‚Äôs formula to find the real roots of the polynomial:\n",
    "\n",
    "- Solve the equation:$a_4 x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0 = 0$\n",
    "- Retain only the real-valued roots (ignore complex ones)\n",
    "\n",
    "Each valid root represents a potential solution (i.e., a value for cos(Œ∏)) that satisfies the P3P geometric constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7331/204400875.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(b, dtype=torch.complex64)\n",
      "/tmp/ipykernel_7331/204400875.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c = torch.tensor(c, dtype=torch.complex64)\n",
      "/tmp/ipykernel_7331/204400875.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  d = torch.tensor(d, dtype=torch.complex64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Computation of the roots\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m roots \u001b[38;5;241m=\u001b[39m \u001b[43mpolynomial_root_calculation_4th_degree_ferrari\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma0\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma2\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma3\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma4\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (4,)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroots = \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, roots)  \u001b[38;5;66;03m# list of tensor (for complex numbers)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m, in \u001b[0;36mpolynomial_root_calculation_4th_degree_ferrari\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     53\u001b[0m b2 \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m S\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Solve the cubic equation m^3 + b2*m^2 + (b2^2/4  - b0)*m - b1^2/8 = 0\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m x_cube \u001b[38;5;241m=\u001b[39m \u001b[43mpolynomial_root_calculation_3rd_degree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb2\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mb0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mb1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Find a real and positive solution\u001b[39;00m\n\u001b[1;32m     61\u001b[0m alpha_0_nul \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mpolynomial_root_calculation_3rd_degree\u001b[0;34m(a, b, c, d)\u001b[0m\n\u001b[1;32m     19\u001b[0m delta_sur_27 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mdelta \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m27\u001b[39m          \u001b[38;5;66;03m# re√©ls\u001b[39;00m\n\u001b[1;32m     21\u001b[0m sqrt_term \u001b[38;5;241m=\u001b[39m sqrt(delta_sur_27)  \u001b[38;5;66;03m# Use the sqrt function defined above\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m u_k \u001b[38;5;241m=\u001b[39m product_of_2_complex_numbers(complex_number_power_k(j_,k), sqrt_3(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39mq\u001b[38;5;241m+\u001b[39m\u001b[43msqrt_term\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m),sqrt_term[\u001b[38;5;241m1\u001b[39m]])) )\u001b[38;5;66;03m# because q real \u001b[39;00m\n\u001b[1;32m     23\u001b[0m v_k \u001b[38;5;241m=\u001b[39m product_of_2_complex_numbers(complex_number_power_k(j_,\u001b[38;5;241m-\u001b[39mk), sqrt_3(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39mq\u001b[38;5;241m-\u001b[39msqrt_term[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39msqrt_term[\u001b[38;5;241m1\u001b[39m]])))\n\u001b[1;32m     25\u001b[0m root \u001b[38;5;241m=\u001b[39m addition(addition(u_k, v_k), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m-\u001b[39mb\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39ma),\u001b[38;5;241m0\u001b[39m]) ) \n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "# Computation of the roots\n",
    "roots = polynomial_root_calculation_4th_degree_ferrari(torch.tensor([a0,a1,a2,a3,a4])) # (4,)\n",
    "\n",
    "print(\"roots = \\n\", roots)  # list of tensor (for complex numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Back-substitute and Recover Pose\n",
    "\n",
    "After solving the quartic polynomial, each real root gives a possible value for $\\cos(\\theta)$. For each such valid root, we reconstruct the corresponding camera pose (position and orientation) using the following steps:\n",
    "1. Recover $\\theta$ by Computing $\\sin(\\theta)$ given a real solution $\\cos(\\theta)$, compute:\n",
    "$\n",
    "\\sin(\\theta) = \\sqrt{1 - \\cos^2(\\theta)}\n",
    "$\n",
    "2. Compute Angle $\\alpha$ via $\\cot(\\alpha)$ from the previous geometric parameters:\n",
    "\n",
    "$$\n",
    "\\cot(\\alpha) = \\frac{ \\left( \\frac{\\phi_1}{\\phi_2} \\right) p_1 + \\cos(\\theta) p_2 - d_{12} b }{ \\left( \\frac{\\phi_1}{\\phi_2} \\right) \\cos(\\theta) p_2 - p_1 + d_{12} }\n",
    "$$\n",
    "\n",
    "Then deduce:\n",
    "\n",
    "$$\n",
    "\\sin(\\alpha) = \\sqrt{ \\frac{1}{\\cot^2(\\alpha) + 1} }, \\quad \n",
    "\\cos(\\alpha) = \\sqrt{ 1 - \\sin^2(\\alpha) }\n",
    "$$\n",
    "\n",
    "> If $ \\cot(\\alpha) < 0 $, then set $ \\cos(\\alpha) := -\\cos(\\alpha) $\n",
    "\n",
    "\n",
    "3. Compute the Camera Center (Intermediate and Absolute) expressed in the world base (intermediate form):\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_{\\text{intermediate}} = d_{12} \\cdot\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\alpha)(\\sin(\\alpha)b + \\cos(\\alpha)) \\\\\n",
    "\\sin(\\alpha)\\cos(\\theta)(\\sin(\\alpha)b + \\cos(\\alpha)) \\\\\n",
    "\\sin(\\alpha)\\sin(\\theta)(\\sin(\\alpha)b + \\cos(\\alpha))\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Transform it back to the global world frame using the world base $\\mathbf{N}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_{\\text{est}} = \\mathbf{P}_1 + \\mathbf{N}^\\top \\cdot \\mathbf{C}_{\\text{intermediate}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "4. Compute the Rotation Matrix\n",
    "\n",
    "Construct the intermediate rotation matrix $ \\mathbf{Q} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q} =\n",
    "\\begin{bmatrix}\n",
    "-\\cos(\\alpha) & -\\sin(\\alpha)\\cos(\\theta) & -\\sin(\\alpha)\\sin(\\theta) \\\\\n",
    "\\sin(\\alpha) & -\\cos(\\alpha)\\cos(\\theta) & -\\cos(\\alpha)\\sin(\\theta) \\\\\n",
    "0 & -\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then compute the full rotation matrix in world coordinates:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\text{est}} = \\mathbf{N}^\\top \\cdot \\mathbf{Q}^\\top \\cdot \\mathbf{T}\n",
    "$$\n",
    "\n",
    "5. Store Solution\n",
    "\n",
    "Each solution is saved in a tensor of shape $(4, 3, 4)$, where:\n",
    "\n",
    "- The **first column** of each slice holds the estimated camera center $\\mathbf{C}_{\\text{est}}$\n",
    "- The **last three columns** form the rotation matrix $\\mathbf{R}_{\\text{est}}$\n",
    "\n",
    "Up to 4 possible solutions are generated. A fourth point $\\mathbf{P}_4, \\mathbf{p}_4$ can later be used to identify the correct one by checking reprojection error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_estimate = \n",
      " tensor([-1.0076,  0.2018,  0.4222], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "Q = \n",
      " tensor([[ 0.9070, -0.1817, -0.3800],\n",
      "        [ 0.4212,  0.3912,  0.8183],\n",
      "        [ 0.0000, -0.9022,  0.4313]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "C_estimate = \n",
      " tensor([0.9832, 0.7517, 2.8388], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "C_estimate = \n",
      " tensor([[0.9832],\n",
      "        [0.7517],\n",
      "        [2.8388]], dtype=torch.float64)\n",
      "C_estimate.shape =  torch.Size([3, 1])\n",
      "R_estimate = \n",
      " tensor([[ 0.7214, -0.5369, -0.4374],\n",
      "        [-0.6541, -0.7357, -0.1756],\n",
      "        [-0.2275,  0.4128, -0.8820]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "C_estimate = \n",
      " tensor([-1.0076,  0.2018,  0.4222], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "Q = \n",
      " tensor([[ 0.9070, -0.1817, -0.3800],\n",
      "        [ 0.4212,  0.3912,  0.8183],\n",
      "        [ 0.0000, -0.9022,  0.4313]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "C_estimate = \n",
      " tensor([0.9832, 0.7517, 2.8388], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "C_estimate = \n",
      " tensor([[0.9832],\n",
      "        [0.7517],\n",
      "        [2.8388]], dtype=torch.float64)\n",
      "C_estimate.shape =  torch.Size([3, 1])\n",
      "R_estimate = \n",
      " tensor([[ 0.7214, -0.5369, -0.4374],\n",
      "        [-0.6541, -0.7357, -0.1756],\n",
      "        [-0.2275,  0.4128, -0.8820]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "C_estimate = \n",
      " tensor([-3.1559,  2.7668,  0.9977], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "Q = \n",
      " tensor([[ 0.7316, -0.6414, -0.2313],\n",
      "        [ 0.6818,  0.6882,  0.2481],\n",
      "        [ 0.0000, -0.3392,  0.9407]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "C_estimate = \n",
      " tensor([    0.0000,     0.0000,     6.0000], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "C_estimate = \n",
      " tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    6.0000]], dtype=torch.float64)\n",
      "C_estimate.shape =  torch.Size([3, 1])\n",
      "R_estimate = \n",
      " tensor([[     1.0000,     -0.0000,     -0.0000],\n",
      "        [    -0.0000,     -1.0000,     -0.0000],\n",
      "        [    -0.0000,      0.0000,     -1.0000]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "C_estimate = \n",
      " tensor([ 6.0281, -1.9115,  0.4390], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "Q = \n",
      " tensor([[-0.9509,  0.3015, -0.0692],\n",
      "        [ 0.3094,  0.9268, -0.2128],\n",
      "        [ 0.0000, -0.2238, -0.9746]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "C_estimate = \n",
      " tensor([-1.2586,  2.2671, -3.9910], dtype=torch.float64)\n",
      "torch.Size([3])\n",
      "C_estimate = \n",
      " tensor([[-1.2586],\n",
      "        [ 2.2671],\n",
      "        [-3.9910]], dtype=torch.float64)\n",
      "C_estimate.shape =  torch.Size([3, 1])\n",
      "R_estimate = \n",
      " tensor([[ 0.8478, -0.5192,  0.1078],\n",
      "        [ 0.5279,  0.8073, -0.2637],\n",
      "        [ 0.0499,  0.2805,  0.9586]], dtype=torch.float64)\n",
      "torch.Size([3, 3])\n",
      "solutions = \n",
      " tensor([[[     0.9832,      0.7214,     -0.5369,     -0.4374],\n",
      "         [     0.7517,     -0.6541,     -0.7357,     -0.1756],\n",
      "         [     2.8388,     -0.2275,      0.4128,     -0.8820]],\n",
      "\n",
      "        [[     0.9832,      0.7214,     -0.5369,     -0.4374],\n",
      "         [     0.7517,     -0.6541,     -0.7357,     -0.1756],\n",
      "         [     2.8388,     -0.2275,      0.4128,     -0.8820]],\n",
      "\n",
      "        [[     0.0000,      1.0000,     -0.0000,     -0.0000],\n",
      "         [     0.0000,     -0.0000,     -1.0000,     -0.0000],\n",
      "         [     6.0000,     -0.0000,      0.0000,     -1.0000]],\n",
      "\n",
      "        [[    -1.2586,      0.8478,     -0.5192,      0.1078],\n",
      "         [     2.2671,      0.5279,      0.8073,     -0.2637],\n",
      "         [    -3.9910,      0.0499,      0.2805,      0.9586]]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/33dbnhh55154_dpx196r4fv80000gn/T/ipykernel_5936/4123210296.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cos_teta = torch.tensor(roots[i][0])# real part of the root\n"
     ]
    }
   ],
   "source": [
    "# For each solution of the polynomial\n",
    "for i in range(4):\n",
    "  #if np.isclose(np.imag(roots[i]),0) : # if real solution \n",
    "\n",
    "    # Computation of trigonometrics forms\n",
    "    cos_teta = torch.tensor(roots[i][0])# real part of the root \n",
    "    sin_teta = torch.sqrt(1-cos_teta**2)\n",
    "\n",
    "    cot_alpha = ((phi1/phi2)*p1 + cos_teta*p2 -d12*b )/ ((phi1/phi2)*cos_teta* p2 - p1 + d12)\n",
    "\n",
    "    sin_alpha = torch.sqrt(1/(cot_alpha**2+1))\n",
    "    cos_alpha= torch.sqrt(1-sin_alpha**2)\n",
    "\n",
    "    if cot_alpha < 0 :\n",
    "      cos_alpha = -cos_alpha\n",
    "\n",
    "    # Computation of the intermediate rotation's matrixs\n",
    "    C_estimate = torch.tensor([d12*cos_alpha*(sin_alpha*b + cos_alpha), d12*sin_alpha*cos_teta*(sin_alpha*b+cos_alpha), d12*sin_alpha*sin_teta*(sin_alpha*b+cos_alpha)]) # (3,)\n",
    "    print(\"C_estimate = \\n\", C_estimate)\n",
    "    print(C_estimate.shape)  # (3,)\n",
    "    Q = torch.tensor([[-cos_alpha, -sin_alpha*cos_teta, -sin_alpha*sin_teta], [sin_alpha, -cos_alpha*cos_teta, -cos_alpha*sin_teta], [0, -sin_teta, cos_teta]])    # (3*3)\n",
    "    print(\"Q = \\n\", Q)\n",
    "    print(Q.shape)  # (3,3)\n",
    "    # Computation of the absolute camera center\n",
    "  \n",
    "    C_estimate = P1 + torch.tensordot(torch.transpose(N, 0,1), C_estimate, dims=1) # (3,)\n",
    "    print(\"C_estimate = \\n\", C_estimate) \n",
    "    print(C_estimate.shape)  # (3,)\n",
    "    C_estimate = torch.reshape(C_estimate, (3,1))  # Reshape to (3,1) for consistency\n",
    "    print(\"C_estimate = \\n\", C_estimate)  # (3,1)\n",
    "    print(\"C_estimate.shape = \", C_estimate.shape)  # (3,1)\n",
    "\n",
    "    # Computation of the orientation matrix\n",
    "    R_estimate = torch.tensordot(torch.tensordot(torch.transpose(N,0,1),torch.transpose(Q, 0,1), dims=1),T,dims=1)   # (3*3)\n",
    "    print(\"R_estimate = \\n\", R_estimate)\n",
    "    print(R_estimate.shape)  # (3,3)\n",
    "    \n",
    "    # Adding C and R to the solutions\n",
    "    solutions[i,:,:1]= C_estimate\n",
    "    solutions[i,:,1:] = R_estimate\n",
    "\n",
    "print(\"solutions = \\n\", solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solutions = \n",
      " tensor([[[     0.9832,      0.7214,     -0.5369,     -0.4374],\n",
      "         [     0.7517,     -0.6541,     -0.7357,     -0.1756],\n",
      "         [     2.8388,     -0.2275,      0.4128,     -0.8820]],\n",
      "\n",
      "        [[     0.9832,      0.7214,     -0.5369,     -0.4374],\n",
      "         [     0.7517,     -0.6541,     -0.7357,     -0.1756],\n",
      "         [     2.8388,     -0.2275,      0.4128,     -0.8820]],\n",
      "\n",
      "        [[     0.0000,      1.0000,     -0.0000,     -0.0000],\n",
      "         [     0.0000,     -0.0000,     -1.0000,     -0.0000],\n",
      "         [     6.0000,     -0.0000,      0.0000,     -1.0000]],\n",
      "\n",
      "        [[    -1.2586,      0.8478,     -0.5192,      0.1078],\n",
      "         [     2.2671,      0.5279,      0.8073,     -0.2637],\n",
      "         [    -3.9910,      0.0499,      0.2805,      0.9586]]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"solutions = \\n\", solutions)\n",
    "print(solutions.shape)  # (4,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. üéØ Step 11 ‚Äî Reprojection and Error Evaluation\n",
    "\n",
    "Once the potential camera poses (C, R) are estimated:\n",
    "\n",
    "1. For each solution (R, C):\n",
    "\n",
    "   - Reconstruct the projection matrix:\n",
    "     $$\n",
    "     P = A \\cdot [R \\mid -RC]\n",
    "     $$\n",
    "   - Project the original 3D points P‚ÇÅ, P‚ÇÇ, P‚ÇÉ onto the image plane:\n",
    "     $$\n",
    "     p_i^{\\text{proj}} = P \\cdot \\tilde{P}_i\n",
    "     $$\n",
    "     where $$\\tilde{P}_i$$ is the homogeneous 3D point.\n",
    "\n",
    "2. Normalize the projected points to get (x, y) pixel coordinates.\n",
    "\n",
    "3. Compute the reprojection error:\n",
    "   $$\n",
    "   \\text{error} = \\| p_i^{\\text{proj}} - p_i^{\\text{observed}} \\|\n",
    "   $$\n",
    "\n",
    "4. Average the reprojection errors across the three points to assess each solution‚Äôs accuracy.\n",
    "\n",
    "‚úÖ This final check helps identify which estimated pose (R, C) best matches the observed 2D data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 =  tensor([[455.7761],\n",
      "        [137.0256]], dtype=torch.float64)\n",
      "torch.Size([2, 1])\n",
      "p2 =  tensor([[397.9924],\n",
      "        [119.7875]], dtype=torch.float64)\n",
      "p3 =  tensor([[549.7900],\n",
      "        [376.1551]], dtype=torch.float64)\n",
      "p4 =  tensor([[123.5423],\n",
      "        [184.6140]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def projection3D2D(point3D,C,R,A) :\n",
    "  # 3D point = [ Xw, Yw, Zw ]'   (1*3)\n",
    "  # T : camera translation matrix : (3*1)\n",
    "  # R : camera rotation matrix : (3*3)\n",
    "  # A : intraseca matrix of the camera : (3*3)\n",
    "  # Output : return the coordonates of the point in 2D \n",
    "\n",
    "  PI = torch.cat((torch.eye(3, dtype=torch.float64),torch.zeros((3,1), dtype=torch.float64)),dim=1)  # (3*4)\n",
    "\n",
    "  Rt = torch.cat((R,C),dim=1)               # (3*4)\n",
    "  Rt = torch.cat((Rt,torch.tensor([[0,0,0,1]], dtype=torch.float64)),dim=0)   # (4*4)\n",
    "\n",
    "  point3D_bis = torch.cat((torch.reshape(point3D,(3,1)),torch.tensor([[1]],dtype=torch.float64)),dim=0)   #(4*1)\n",
    "  \n",
    "  point2D = torch.tensordot(torch.tensordot(torch.tensordot(A,PI,dims=1),Rt,dims=1),point3D_bis,dims=1)  # 2D point = [u, v, w] (3*1)\n",
    "  point2D = point2D / point2D[2]        # 2D point = [u, v, 1] (3*1)\n",
    "  return point2D[:2]\n",
    "\n",
    "\n",
    "C_transpose = torch.transpose(C, 0, 1)  # (3*1) -> (1*3)\n",
    "\n",
    "p1 = projection3D2D(P1,C_transpose,R,A)\n",
    "print(\"p1 = \", p1)\n",
    "print(p1.shape)  # (2,1)\n",
    "\n",
    "p2 = projection3D2D(points3D[1],C_transpose,R,A)\n",
    "print(\"p2 = \", p2)\n",
    "p3 = projection3D2D(points3D[2],C_transpose,R,A)\n",
    "print(\"p3 = \", p3)\n",
    "P4 = torch.tensor([-1.5224, 0.4292, -0.1994], dtype=torch.float64) \n",
    "p4 = projection3D2D(P4,C_transpose,R,A)\n",
    "print(\"p4 = \", p4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(pt, pt_estimation):\n",
    "    erreur = torch.tensor(0, dtype=torch.float64)  # Initialize error as a tensor\n",
    "    for i in range(len(pt)): \n",
    "      erreur = erreur + torch.tensor((pt[i] - pt_estimation[i])**2, dtype=torch.float64)  # Ensure each term is a tensor\n",
    "      #erreur += (pt[i] - pt_estimation[i])**2\n",
    "    return torch.sqrt(erreur)\n",
    "\n",
    "\n",
    "\n",
    "def affichage_erreur(solutions,points2D,points3D,A) : \n",
    "   # Compute the error of estimation for each points after the P3P algorithm \n",
    "\n",
    "   # solutions : solution matrix returned by P3P (4*3*4)\n",
    "   # points 3D : 4 pts 3D used for P3P \n",
    "   # points 2D : 4 pts 2D used for P3P (image of the 3D points)\n",
    "   \n",
    "   P1 = points3D[0]\n",
    "   P2 = points3D[1]\n",
    "   P3 = points3D[2]\n",
    "   P4 = points3D[3]\n",
    "\n",
    "   erreurs = []\n",
    "   nb_sol = 0\n",
    "\n",
    "   for i in range(len(solutions)) : \n",
    "      R = solutions[i,:,1:] \n",
    "      C = solutions[i,:,:1]\n",
    "\n",
    "      if not torch.all(R==torch.zeros((3,3))) : \n",
    "        nb_sol += 1 \n",
    "        print(\"------------ Solution n¬∞ : \",nb_sol,\"----------------\")\n",
    "        print(\"R = \\n\",R,)\n",
    "        print(\"T = \\n\",C,)\n",
    "\n",
    "        p1_P3P = torch.reshape(projection3D2D(P1,C,R,A),(1,2))\n",
    "        p2_P3P = torch.reshape(projection3D2D(P2,C,R,A),(1,2))\n",
    "        p3_P3P = torch.reshape(projection3D2D(P3,C,R,A),(1,2))\n",
    "        p4_P3P = torch.reshape(projection3D2D(P4,C,R,A),(1,2))\n",
    "        pt_2D_P3P = torch.cat((p1_P3P,p2_P3P,p3_P3P,p4_P3P),dim=0)    # (4,2)\n",
    "\n",
    "        erreurs.append([0])\n",
    "        for j in range(len(points2D)):\n",
    "            erreur_pt = distance(points2D[j],pt_2D_P3P[j])\n",
    "            erreurs[i]+=erreur_pt\n",
    "        \n",
    "   indice_min = 0\n",
    "   min = erreurs[0]\n",
    "   for i in range(1,len(erreurs)) :\n",
    "    if erreurs[i]<min :\n",
    "      min = erreurs[i]\n",
    "      indice_min = i\n",
    "\n",
    "   R_opti = solutions[indice_min,:,1:] \n",
    "   C_opti = solutions[indice_min,:,:1]\n",
    "   print(\"\\n------------ Best solution : ----------------\")\n",
    "   print(\"Solution n¬∞ :\",indice_min+1,\"\\n\")\n",
    "   print(\"R estim√© = \\n\", R_opti,\"\\n\")\n",
    "   print(\"T estim√© = \\n\", C_opti, \"\\n\")\n",
    "   print(\"Erreur = \", erreurs[indice_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Solution n¬∞ :  1 ----------------\n",
      "R = \n",
      " tensor([[-0.9750,  0.0867, -0.2048],\n",
      "        [ 0.1565,  0.9217, -0.3550],\n",
      "        [ 0.1580, -0.3782, -0.9122]], dtype=torch.float64)\n",
      "T = \n",
      " tensor([[ 5.2156],\n",
      "        [ 7.7809],\n",
      "        [18.0302]], dtype=torch.float64)\n",
      "------------ Solution n¬∞ :  2 ----------------\n",
      "R = \n",
      " tensor([[-0.9750,  0.0867, -0.2048],\n",
      "        [ 0.1565,  0.9217, -0.3550],\n",
      "        [ 0.1580, -0.3782, -0.9122]], dtype=torch.float64)\n",
      "T = \n",
      " tensor([[ 5.2156],\n",
      "        [ 7.7809],\n",
      "        [18.0303]], dtype=torch.float64)\n",
      "------------ Solution n¬∞ :  3 ----------------\n",
      "R = \n",
      " tensor([[-0.9653,  0.1073, -0.2380],\n",
      "        [ 0.1832,  0.9279, -0.3247],\n",
      "        [ 0.1861, -0.3570, -0.9154]], dtype=torch.float64)\n",
      "T = \n",
      " tensor([[ 5.3984],\n",
      "        [ 6.6417],\n",
      "        [16.6938]], dtype=torch.float64)\n",
      "------------ Solution n¬∞ :  4 ----------------\n",
      "R = \n",
      " tensor([[-0.6472,  0.4623,  0.6062],\n",
      "        [-0.7622, -0.3773, -0.5260],\n",
      "        [-0.0145, -0.8024,  0.5966]], dtype=torch.float64)\n",
      "T = \n",
      " tensor([[-11.1975],\n",
      "        [ 11.9651],\n",
      "        [-11.7825]], dtype=torch.float64)\n",
      "\n",
      "------------ Best solution : ----------------\n",
      "Solution n¬∞ : 3 \n",
      "\n",
      "R estim√© = \n",
      " tensor([[-0.9653,  0.1073, -0.2380],\n",
      "        [ 0.1832,  0.9279, -0.3247],\n",
      "        [ 0.1861, -0.3570, -0.9154]], dtype=torch.float64) \n",
      "\n",
      "T estim√© = \n",
      " tensor([[ 5.3984],\n",
      "        [ 6.6417],\n",
      "        [16.6938]], dtype=torch.float64) \n",
      "\n",
      "Erreur =  [0, tensor(470.0183, dtype=torch.float64), tensor(635.9902, dtype=torch.float64), tensor(660.7983, dtype=torch.float64), tensor(660.7983, dtype=torch.float64)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1011/3400443864.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  erreur = erreur + torch.tensor((pt[i] - pt_estimation[i])**2, dtype=torch.float64)  # Ensure each term is a tensor\n"
     ]
    }
   ],
   "source": [
    "affichage_erreur(solutions, [p1, p2, p3, p4], [P1,P2,P3, P4], A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolonas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
