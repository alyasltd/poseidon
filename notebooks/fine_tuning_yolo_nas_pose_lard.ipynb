{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fine-tuning YOLO-NAS-POSE on the LARD Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Image not found for label DIAP_21_500_269.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON sauvegardé dans /home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_annotations.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "image_base = Path(\"/home/aws_install/data/yolonas_pose_base/images\")\n",
    "label_base = Path(\"/home/aws_install/data/yolonas_pose_base/labels\")\n",
    "output_json_path = \"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_annotations.json\"\n",
    "\n",
    "categories = [{\n",
    "    \"id\": 0,\n",
    "    \"name\": \"runway\",\n",
    "    \"keypoints\": [\"A\", \"B\", \"C\", \"D\"], \n",
    "    \"skeleton\": [\n",
    "        [0, 1],\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0]\n",
    "    ]\n",
    "}]\n",
    "\n",
    "annotation_id = 1\n",
    "image_id = 1\n",
    "json_data = {\n",
    "    \"info\": {},\n",
    "    \"categories\": categories,\n",
    "    \"images\": [],\n",
    "    \"annotations\": []\n",
    "}\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_image_folder = image_base / split\n",
    "    split_label_folder = label_base / split\n",
    "\n",
    "    for label_path in split_label_folder.glob(\"*.txt\"):\n",
    "        image_name = None\n",
    "        image_path = None\n",
    "\n",
    "        for ext in [\".jpeg\", \".png\"]:\n",
    "            candidate_name = label_path.stem + ext\n",
    "            candidate_path = split_image_folder / candidate_name\n",
    "            if candidate_path.exists():\n",
    "                image_name = candidate_name\n",
    "                image_path = candidate_path\n",
    "                break\n",
    "\n",
    "        if image_path is None or not image_path.exists():\n",
    "            print(f\"⚠️ Image not found for label {label_path.name}\")\n",
    "            continue\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            line = f.readline().strip()\n",
    "            parts = line.split()\n",
    "\n",
    "        if len(parts) < 5:\n",
    "            print(f\"⚠️ Ligne d’annotation trop courte dans {label_path.name}\")\n",
    "            continue\n",
    "\n",
    "        class_id = int(parts[0])\n",
    "        x_center, y_center, width, height = map(float, parts[1:5])\n",
    "\n",
    "        x1 = x_center - width / 2\n",
    "        y1 = y_center - height / 2\n",
    "\n",
    "        # === Keypoints extraction ===\n",
    "        keypoints = []\n",
    "        for i in range(5, len(parts), 3):\n",
    "            try:\n",
    "                x, y, v = float(parts[i]), float(parts[i+1]), int(parts[i+2])\n",
    "                keypoints.extend([x, y, v])\n",
    "            except:\n",
    "                keypoints.extend([0, 0, 0])  # fallback si ligne incomplète\n",
    "\n",
    "        image_dict = {\n",
    "            \"file_name\": str(image_path.relative_to(image_base)),\n",
    "\n",
    "            \"id\": image_id\n",
    "        }\n",
    "        json_data[\"images\"].append(image_dict)\n",
    "\n",
    "        annotation = {\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": class_id,\n",
    "            \"bbox\": [x1, y1, width, height],\n",
    "            \"keypoints\": keypoints\n",
    "        }\n",
    "\n",
    "        json_data[\"annotations\"].append(annotation)\n",
    "        annotation_id += 1\n",
    "        image_id += 1\n",
    "\n",
    "# saving the JSON data\n",
    "os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ JSON sauvegardé dans {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from super_gradients.common.decorators.factory_decorator import resolve_param\n",
    "from super_gradients.common.factories.transforms_factory import TransformsFactory\n",
    "from super_gradients.training.transforms.keypoint_transforms import AbstractKeypointTransform\n",
    "from super_gradients.training.samples import PoseEstimationSample\n",
    "from super_gradients.training.datasets.pose_estimation_datasets.abstract_pose_estimation_dataset import AbstractPoseEstimationDataset\n",
    "\n",
    "from super_gradients.training.utils.distributed_training_utils import wait_for_the_master\n",
    "from super_gradients.common.environment.ddp_utils import get_local_rank\n",
    "from super_gradients.training.datasets.pose_estimation_datasets import YoloNASPoseCollateFN\n",
    "\n",
    "\n",
    "class RunwayPoseEstimationDataset(AbstractPoseEstimationDataset):\n",
    "    @classmethod\n",
    "    def split_runway_pose_dataset(cls, annotation_file, train_annotation_file, val_annotation_file, val_fraction):\n",
    "        \"\"\"\n",
    "        Splits the runway pose dataset into training and validation sets.\n",
    "        :param annotation_file: Path to the original annotation file.\n",
    "        :param train_annotation_file: Path to save the training annotations.\n",
    "        :param val_annotation_file: Path to save the validation annotations.\n",
    "        :param val_fraction: Fraction of the dataset to be used for validation.\n",
    "        \"\"\"\n",
    "        with open(annotation_file, \"r\") as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        image_ids = [img[\"id\"] for img in annotation[\"images\"]]\n",
    "        labels = [[ann[\"category_id\"] for ann in annotation[\"annotations\"] if ann[\"image_id\"] == img_id] for img_id in image_ids]\n",
    "        labels = [label[0] if len(label) else -1 for label in labels]\n",
    "\n",
    "        train_ids, val_ids = train_test_split(image_ids, test_size=val_fraction, random_state=42, stratify=labels)\n",
    "\n",
    "        train_annotations = {\n",
    "            \"info\": annotation.get(\"info\", {}),\n",
    "            \"categories\": annotation[\"categories\"],\n",
    "            \"images\": [img for img in annotation[\"images\"] if img[\"id\"] in train_ids],\n",
    "            \"annotations\": [ann for ann in annotation[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "        }\n",
    "\n",
    "        val_annotations = {\n",
    "            \"info\": annotation.get(\"info\", {}),\n",
    "            \"categories\": annotation[\"categories\"],\n",
    "            \"images\": [img for img in annotation[\"images\"] if img[\"id\"] in val_ids],\n",
    "            \"annotations\": [ann for ann in annotation[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "        }\n",
    "\n",
    "        with open(train_annotation_file, \"w\") as f:\n",
    "            json.dump(train_annotations, f, indent=2)\n",
    "        with open(val_annotation_file, \"w\") as f:\n",
    "            json.dump(val_annotations, f, indent=2)\n",
    "\n",
    "    @resolve_param(\"transforms\", TransformsFactory())\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        images_dir: str,\n",
    "        json_file: str,\n",
    "        transforms: List[AbstractKeypointTransform],\n",
    "        edge_links: Union[List[Tuple[int, int]], np.ndarray],\n",
    "        edge_colors: Union[List[Tuple[int, int, int]], np.ndarray, None],\n",
    "        keypoint_colors: Union[List[Tuple[int, int, int]], np.ndarray, None],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param data_dir: Root directory of the COCO dataset\n",
    "        :param images_dir: path suffix to the images directory inside the data_dir\n",
    "        :param json_file: path suffix to the json file inside the data_dir\n",
    "        :param include_empty_samples: Not used, but exists for compatibility with COCO dataset config.\n",
    "        :param target_generator: Target generator that will be used to generate the targets for the model.\n",
    "            See DEKRTargetsGenerator for an example.\n",
    "        :param transforms: Transforms to be applied to the image & keypoints\n",
    "        \"\"\"\n",
    "        json_path = os.path.join(data_dir, json_file)\n",
    "        with open(json_path, \"r\") as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        joints = annotation[\"categories\"][0][\"keypoints\"]\n",
    "        num_joints = len(joints)\n",
    "\n",
    "        super().__init__( \n",
    "            transforms=transforms,\n",
    "            num_joints=num_joints,\n",
    "            edge_links=edge_links,\n",
    "            edge_colors=edge_colors,\n",
    "            keypoint_colors=keypoint_colors,\n",
    "        )\n",
    "\n",
    "        self.image_id_to_file = {img[\"id\"]: os.path.join(data_dir, images_dir, img[\"file_name\"]) for img in annotation[\"images\"]}\n",
    "        self.image_ids = list(self.image_id_to_file.keys())\n",
    "        self.image_files = list(self.image_id_to_file.values())\n",
    "\n",
    "        self.annotations = []\n",
    "        for image_id in self.image_ids:\n",
    "            anns = [ann for ann in annotation[\"annotations\"] if ann[\"image_id\"] == image_id]\n",
    "            keypoints_list = []\n",
    "            bboxes_list = []\n",
    "            for ann in anns:\n",
    "                kpts = np.array(ann[\"keypoints\"]).reshape(num_joints, 3)\n",
    "                x, y, w, h = ann[\"bbox\"]\n",
    "                keypoints_list.append(kpts)\n",
    "                bboxes_list.append(np.array([x, y, w, h]))\n",
    "            if keypoints_list:\n",
    "                self.annotations.append((np.array(keypoints_list, dtype=np.float32), np.array(bboxes_list, dtype=np.float32)))\n",
    "            else:\n",
    "                self.annotations.append((np.zeros((0, num_joints, 3)), np.zeros((0, 4))))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        :return: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def load_sample(self, index) -> PoseEstimationSample:\n",
    "        \"\"\"\n",
    "        Loads a sample from the dataset.\n",
    "        :param index: Index of the sample to load.\n",
    "        :return: PoseEstimationSample object containing the image, mask, joints, areas, bounding boxes, and is_crowd.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.image_files[index])\n",
    "        joints, bboxes = self.annotations[index]\n",
    "        areas = np.array([w * h for (_, _, w, h) in bboxes], dtype=np.float32)\n",
    "        iscrowd = np.zeros(len(joints), dtype=bool)\n",
    "        mask = np.ones(image.shape[:2], dtype=np.float32)\n",
    "        return PoseEstimationSample(image=image, mask=mask, joints=joints, areas=areas, bboxes_xywh=bboxes, is_crowd=iscrowd, additional_samples=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunwayPoseEstimationDataset.split_runway_pose_dataset(\n",
    "    annotation_file=\"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_annotations.json\",\n",
    "    train_annotation_file=\"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_train.json\",\n",
    "    val_annotation_file=\"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_val.json\",\n",
    "    val_fraction=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Vérification du fichier : Complet\n",
      " - Nombre d'images : 16747\n",
      " - Nombre d'annotations : 16747\n",
      " - Nombre de catégories : 1\n",
      " - Keypoints par annotation :\n",
      "   > Min : 12\n",
      "   > Max : 12\n",
      "   > Moyenne : 12.00\n",
      "\n",
      "📂 Vérification du fichier : Train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Nombre d'images : 13397\n",
      " - Nombre d'annotations : 13397\n",
      " - Nombre de catégories : 1\n",
      " - Keypoints par annotation :\n",
      "   > Min : 12\n",
      "   > Max : 12\n",
      "   > Moyenne : 12.00\n",
      "\n",
      "📂 Vérification du fichier : Val\n",
      " - Nombre d'images : 3350\n",
      " - Nombre d'annotations : 3350\n",
      " - Nombre de catégories : 1\n",
      " - Keypoints par annotation :\n",
      "   > Min : 12\n",
      "   > Max : 12\n",
      "   > Moyenne : 12.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "annotation_files = {\n",
    "    \"Complet\": \"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_annotations.json\",\n",
    "    \"Train\": \"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_train.json\",\n",
    "    \"Val\": \"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_val.json\"\n",
    "}\n",
    "\n",
    "for label, path in annotation_files.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠️ Fichier manquant : {path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📂 Vérification du fichier : {label}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(\" - Nombre d'images :\", len(data[\"images\"]))\n",
    "    print(\" - Nombre d'annotations :\", len(data[\"annotations\"]))\n",
    "    print(\" - Nombre de catégories :\", len(data[\"categories\"]))\n",
    "\n",
    "    keypoint_lengths = [len(ann[\"keypoints\"]) for ann in data[\"annotations\"]]\n",
    "    if keypoint_lengths:\n",
    "        print(\" - Keypoints par annotation :\")\n",
    "        print(f\"   > Min : {min(keypoint_lengths)}\")\n",
    "        print(f\"   > Max : {max(keypoint_lengths)}\")\n",
    "        print(f\"   > Moyenne : {sum(keypoint_lengths)/len(keypoint_lengths):.2f}\")\n",
    "    else:\n",
    "        print(\" - Pas de keypoints trouvés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINT_NAMES = [\n",
    "    \"corner_top_left\",\n",
    "    \"corner_top_right\",\n",
    "    \"corner_bottom_right\",\n",
    "    \"corner_bottom_left\",\n",
    "]\n",
    "\n",
    "FLIP_INDEXES = [\n",
    "    1, 0, 3, 2\n",
    "]\n",
    "\n",
    "EDGE_LINKS = [\n",
    "    [0, 1],\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 0],\n",
    "]\n",
    "\n",
    "EDGE_COLORS = [[0, 255, 0]] * len(EDGE_LINKS)\n",
    "KEYPOINT_COLORS = [[255, 0, 0]] * len(KEYPOINT_NAMES)\n",
    "NUM_JOINTS = len(KEYPOINT_NAMES)\n",
    "OKS_SIGMAS = [0.07] * NUM_JOINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training.transforms.keypoints import (\n",
    "    KeypointsRandomHorizontalFlip,\n",
    "    KeypointsHSV,\n",
    "    KeypointsBrightnessContrast,\n",
    "    KeypointsRandomAffineTransform,\n",
    "    KeypointsLongestMaxSize,\n",
    "    KeypointsPadIfNeeded,\n",
    "    KeypointsImageStandardize,\n",
    "    KeypointsRemoveSmallObjects,\n",
    ")\n",
    "\n",
    "IMAGE_SIZE = 640\n",
    "\n",
    "# Define the transforms for training and validation datasets\n",
    "train_transforms = [\n",
    "    KeypointsRandomHorizontalFlip(flip_index=FLIP_INDEXES, prob=0.5), # Random horizontal flip with specified keypoint flip indexes\n",
    "    KeypointsHSV(prob=0.5, hgain=20, sgain=20, vgain=20), # Random HSV adjustments\n",
    "    KeypointsBrightnessContrast(prob=0.5, brightness_range=[0.8, 1.2], contrast_range=[0.8, 1.2]), # Random brightness and contrast adjustments\n",
    "    KeypointsRandomAffineTransform(\n",
    "        max_rotation=15,  # Maximum rotation in degrees\n",
    "        min_scale=0.8,  # Minimum scale factor\n",
    "        max_scale=1.2, # Maximum scale factor\n",
    "        max_translate=0.1, # Maximum translation as a fraction of the image size\n",
    "        image_pad_value=127,    # Padding value for the image\n",
    "        mask_pad_value=1, # Padding value for the mask\n",
    "        prob=0.75, # Probability of applying the affine transformation\n",
    "        interpolation_mode=[0, 1, 2, 3, 4], # Interpolation modes to choose from\n",
    "    ),\n",
    "    KeypointsLongestMaxSize(max_height=IMAGE_SIZE, max_width=IMAGE_SIZE), # Resize the image to the longest side with a maximum size\n",
    "    KeypointsPadIfNeeded( \n",
    "        min_height=IMAGE_SIZE, # Minimum height after padding\n",
    "        min_width=IMAGE_SIZE, # Minimum width after padding\n",
    "        image_pad_value=[127, 127, 127], # Padding value for the image\n",
    "        mask_pad_value=1, # Padding value for the mask\n",
    "        padding_mode=\"bottom_right\", # Padding mode to use\n",
    "    ),\n",
    "    KeypointsImageStandardize(max_value=255),\n",
    "    KeypointsRemoveSmallObjects(min_instance_area=1, min_visible_keypoints=1),\n",
    "]\n",
    "\n",
    "val_transforms = [\n",
    "    KeypointsLongestMaxSize(max_height=IMAGE_SIZE, max_width=IMAGE_SIZE),\n",
    "    KeypointsPadIfNeeded(\n",
    "        min_height=IMAGE_SIZE,\n",
    "        min_width=IMAGE_SIZE,\n",
    "        image_pad_value=[127, 127, 127],\n",
    "        mask_pad_value=1,\n",
    "        padding_mode=\"bottom_right\",\n",
    "    ),\n",
    "    KeypointsImageStandardize(max_value=255),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RunwayPoseEstimationDataset(\n",
    "    data_dir=\"/home/aws_install/data/yolonas_pose_base\",     # Root directory of the dataset\n",
    "    images_dir=\"/home/aws_install/data/yolonas_pose_base/images\",\n",
    "    json_file=\"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_train.json\",\n",
    "    transforms=train_transforms, \n",
    "    edge_links=EDGE_LINKS,\n",
    "    edge_colors=EDGE_COLORS,\n",
    "    keypoint_colors=KEYPOINT_COLORS,\n",
    ")\n",
    "\n",
    "val_dataset = RunwayPoseEstimationDataset(\n",
    "    data_dir=\"/home/aws_install/data/yolonas_pose_base\",     # Root directory of the dataset\n",
    "    images_dir=\"/home/aws_install/data/yolonas_pose_base/images\",\n",
    "    json_file=\"/home/aws_install/data/yolonas_pose_base/annotations/yolonas_pose_val.json\",\n",
    "    transforms=val_transforms,\n",
    "    edge_links=EDGE_LINKS,\n",
    "    edge_colors=EDGE_COLORS,\n",
    "    keypoint_colors=KEYPOINT_COLORS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader_params = {\"shuffle\": True, \"batch_size\": 24, \"drop_last\": True, \"pin_memory\": False, \"collate_fn\": YoloNASPoseCollateFN()}\n",
    "\n",
    "val_dataloader_params = {\"shuffle\": True, \"batch_size\": 24, \"drop_last\": True, \"pin_memory\": False, \"collate_fn\": YoloNASPoseCollateFN()}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **train_dataloader_params)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, **val_dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training.models.pose_estimation_models.yolo_nas_pose import YoloNASPosePostPredictionCallback\n",
    "from super_gradients.training.utils.callbacks import ExtremeBatchPoseEstimationVisualizationCallback, Phase\n",
    "from super_gradients.training.utils.early_stopping import EarlyStop\n",
    "from super_gradients.training.metrics import PoseEstimationMetrics\n",
    "\n",
    "post_prediction_callback = YoloNASPosePostPredictionCallback(\n",
    "    pose_confidence_threshold=0.01,\n",
    "    nms_iou_threshold=0.7,\n",
    "    pre_nms_max_predictions=300,\n",
    "    post_nms_max_predictions=30,\n",
    ")\n",
    "\n",
    "metrics = PoseEstimationMetrics(\n",
    "    num_joints=NUM_JOINTS,\n",
    "    oks_sigmas=OKS_SIGMAS,\n",
    "    max_objects_per_image=30,\n",
    "    post_prediction_callback=post_prediction_callback,\n",
    ")\n",
    "\n",
    "visualization_callback = ExtremeBatchPoseEstimationVisualizationCallback(\n",
    "    keypoint_colors=KEYPOINT_COLORS,\n",
    "    edge_colors=EDGE_COLORS,\n",
    "    edge_links=EDGE_LINKS,\n",
    "    loss_to_monitor=\"YoloNASPoseLoss/loss\",\n",
    "    max=True,\n",
    "    freq=1,\n",
    "    max_images=16,\n",
    "    enable_on_train_loader=True,\n",
    "    enable_on_valid_loader=True,\n",
    "    post_prediction_callback=post_prediction_callback,\n",
    ")\n",
    "\n",
    "early_stop = EarlyStop(\n",
    "    phase=Phase.VALIDATION_EPOCH_END,\n",
    "    monitor=\"AP\",\n",
    "    mode=\"max\",\n",
    "    min_delta=0.0001,\n",
    "    patience=100,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_params = {\n",
    "    \"warmup_mode\": \"LinearBatchLRWarmup\",\n",
    "    \"warmup_initial_lr\": 1e-8,\n",
    "    \"lr_warmup_epochs\": 2,\n",
    "    \"initial_lr\": 5e-4,\n",
    "    \"lr_mode\": \"cosine\",\n",
    "    \"cosine_final_lr_ratio\": 0.05,\n",
    "    \"max_epochs\": 10,\n",
    "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
    "    \"batch_accumulate\": 1,\n",
    "    \"average_best_models\": True,\n",
    "    \"save_ckpt_epoch_list\": [],\n",
    "    \"loss\": \"yolo_nas_pose_loss\",\n",
    "    \"criterion_params\": {\n",
    "        \"oks_sigmas\": OKS_SIGMAS,\n",
    "        \"classification_loss_weight\": 1.0,\n",
    "        \"classification_loss_type\": \"focal\",\n",
    "        \"regression_iou_loss_type\": \"ciou\",\n",
    "        \"iou_loss_weight\": 2.5,\n",
    "        \"dfl_loss_weight\": 0.01,\n",
    "        \"pose_cls_loss_weight\": 1.0,\n",
    "        \"pose_reg_loss_weight\": 34.0,\n",
    "        \"pose_classification_loss_type\": \"focal\",\n",
    "        \"rescale_pose_loss_with_assigned_score\": True,\n",
    "        \"assigner_multiply_by_pose_oks\": True,\n",
    "    },\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"optimizer_params\": {\"weight_decay\": 0.000001},\n",
    "    \"ema\": True,\n",
    "    \"ema_params\": {\"decay\": 0.997, \"decay_type\": \"threshold\"},\n",
    "    \"mixed_precision\": True,\n",
    "    \"sync_bn\": False,\n",
    "    \"valid_metrics_list\": [metrics],\n",
    "    \"phase_callbacks\": [visualization_callback, early_stop],\n",
    "    \"pre_prediction_callback\": None,\n",
    "    \"metric_to_watch\": \"AP\",\n",
    "    \"greater_metric_to_watch_is_better\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-24 23:43:49] WARNING - checkpoint_utils.py - :warning: The pre-trained models provided by SuperGradients may have their own licenses or terms and conditions derived from the dataset used for pre-training.\n",
      " It is your responsibility to determine whether you have permission to use the models for your use case.\n",
      " The model you have requested was pre-trained on the coco_pose dataset, published under the following terms: https://cocodataset.org/#termsofuse\n",
      "[2025-06-24 23:43:49] INFO - checkpoint_utils.py - License Notification: YOLO-NAS-POSE pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
      "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS-POSE.md\n",
      "By downloading the pre-trained weight files you agree to comply with these terms.\n",
      "[2025-06-24 23:43:49] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_pose_m\n",
      "[2025-06-24 23:43:49] INFO - sg_trainer.py - Starting a new run with `run_id=RUN_20250624_234349_753258`\n",
      "[2025-06-24 23:43:49] INFO - sg_trainer.py - Checkpoints directory: checkpoints/lard_ft_1/RUN_20250624_234349_753258\n",
      "[2025-06-24 23:43:49] INFO - sg_trainer.py - Using EMA with params {'decay': 0.997, 'decay_type': 'threshold'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is now moved to checkpoints/lard_ft_1/RUN_20250624_234349_753258/console_Jun24_23_43_49.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aws_install/super-gradients/src/super_gradients/training/sg_trainer/sg_trainer.py:1765: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(enabled=mixed_precision_enabled)\n",
      "[2025-06-24 23:44:00] INFO - sg_trainer_utils.py - TRAINING PARAMETERS:\n",
      "    - Mode:                         Single GPU\n",
      "    - Number of GPUs:               1          (1 available on the machine)\n",
      "    - Full dataset size:            13397      (len(train_set))\n",
      "    - Batch size per GPU:           24         (batch_size)\n",
      "    - Batch Accumulate:             1          (batch_accumulate)\n",
      "    - Total batch size:             24         (num_gpus * batch_size)\n",
      "    - Effective Batch size:         24         (num_gpus * batch_size * batch_accumulate)\n",
      "    - Iterations per epoch:         558        (len(train_loader))\n",
      "    - Gradient updates per epoch:   558        (len(train_loader) / batch_accumulate)\n",
      "    - Model: YoloNASPose_M  (58.17M parameters, 58.17M optimized)\n",
      "    - Learning Rates and Weight Decays:\n",
      "      - default: (58.17M parameters). LR: 0.0005 (58.17M parameters) WD: 0.0, (78.17K parameters), WD: 1e-06, (58.09M parameters)\n",
      "\n",
      "[2025-06-24 23:44:00] INFO - sg_trainer.py - Started training for 10 epochs (0/9)\n",
      "\n",
      "Train epoch 0:   0%|          | 0/558 [00:00<?, ?it/s]/home/aws_install/super-gradients/src/super_gradients/training/sg_trainer/sg_trainer.py:503: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.training_params.mixed_precision):\n",
      "Train epoch 0:  21%|██        | 115/558 [21:04<1:16:46, 10.40s/it, YoloNASPoseLoss/loss=1.73e+3, YoloNASPoseLoss/loss_cls=1.73e+3, YoloNASPoseLoss/loss_dfl=0, YoloNASPoseLoss/loss_iou=0, YoloNASPoseLoss/loss_pose_cls=0, YoloNASPoseLoss/loss_pose_reg=0, gpu_mem=15.2]Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 244, in run\n",
      "    self._run()\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 275, in _run\n",
      "    self._record_writer.write(data)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 775, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 167, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 171, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'checkpoints/lard_ft_1/RUN_20250624_234130_869442/events.out.tfevents.1750801290.ip-10-102-214-245.2k37-leapoc.aws.cloud.airbus.corp.14891.1'\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 244, in run\n",
      "    self._run()\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 275, in _run\n",
      "    self._record_writer.write(data)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 775, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 167, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 171, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'checkpoints/lard_ft_1/RUN_20250624_234231_707190/events.out.tfevents.1750801351.ip-10-102-214-245.2k37-leapoc.aws.cloud.airbus.corp.14891.2'\n",
      "Train epoch 0:  21%|██        | 117/558 [21:27<1:19:58, 10.88s/it, YoloNASPoseLoss/loss=1.7e+3, YoloNASPoseLoss/loss_cls=1.7e+3, YoloNASPoseLoss/loss_dfl=0, YoloNASPoseLoss/loss_iou=0, YoloNASPoseLoss/loss_pose_cls=0, YoloNASPoseLoss/loss_pose_reg=0, gpu_mem=15.2]  Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 244, in run\n",
      "    self._run()\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 275, in _run\n",
      "    self._record_writer.write(data)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 775, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 167, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"/home/aws_install/miniconda3/envs/yolonas/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 171, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'checkpoints/lard_ft_1/RUN_20250624_233506_187030/events.out.tfevents.1750800906.ip-10-102-214-245.2k37-leapoc.aws.cloud.airbus.corp.14891.0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 0:  21%|██        | 117/558 [21:39<1:21:37, 11.11s/it, YoloNASPoseLoss/loss=1.7e+3, YoloNASPoseLoss/loss_cls=1.7e+3, YoloNASPoseLoss/loss_dfl=0, YoloNASPoseLoss/loss_iou=0, YoloNASPoseLoss/loss_pose_cls=0, YoloNASPoseLoss/loss_pose_reg=0, gpu_mem=15.2]\n",
      "[2025-06-25 00:05:39] INFO - sg_trainer.py - \n",
      "[MODEL TRAINING EXECUTION HAS BEEN INTERRUPTED]... Please wait until SOFT-TERMINATION process finishes and saves all of the Model Checkpoints and log files before terminating...\n",
      "[2025-06-25 00:05:39] INFO - sg_trainer.py - For HARD Termination - Stop the process again\n",
      "[2025-06-25 00:05:39] INFO - base_sg_logger.py - [CLEANUP] - Successfully stopped system monitoring process\n"
     ]
    }
   ],
   "source": [
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import Trainer\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "trainer = Trainer(experiment_name=\"lard_ft_1\", ckpt_root_dir=CHECKPOINT_DIR)\n",
    "\n",
    "yolo_nas_pose = models.get(Models.YOLO_NAS_POSE_M, num_classes=1, pretrained_weights=\"coco_pose\").cuda()\n",
    "\n",
    "# Note, this is training for 10 epochs to demonstrate how to do it\n",
    "trainer.train(model=yolo_nas_pose, training_params=train_params, train_loader=train_dataloader, valid_loader=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_prediction_callback = YoloNASPosePostPredictionCallback(\n",
    "    pose_confidence_threshold=0.01,\n",
    "    nms_iou_threshold=0.7,\n",
    "    pre_nms_max_predictions=300,\n",
    "    post_nms_max_predictions=30,\n",
    ")\n",
    "\n",
    "metrics = PoseEstimationMetrics(\n",
    "    num_joints=NUM_JOINTS,\n",
    "    oks_sigmas=OKS_SIGMAS,\n",
    "    max_objects_per_image=30,\n",
    "    post_prediction_callback=post_prediction_callback,\n",
    ")\n",
    "\n",
    "trainer.test(model=best_model, test_loader=test_dataloader, test_metrics_list=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"content/images/2007_000783.jpg\"\n",
    "best_model.predict(img_url, conf=0.20, fuse_model=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolonas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
